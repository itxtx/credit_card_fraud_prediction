{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import your existing utility module\n",
    "import credit_card_fraud_utils as ccf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "X_train, X_test, y_train, y_test = test_train_split(df)\n",
    "\n",
    "\n",
    "\n",
    "model = IsolationForest(contamination=0.001727, random_state=42, n_estimators=100, max_samples='auto')\n",
    "model.fit(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions from (-1, 1) to (0, 1) format\n",
    "y_pred = np.where(y_pred == -1, 1, 0)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "auprc = auc(recall, precision)\n",
    "print(f\"AUPRC: {auprc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get anomaly scores (the lower the score, the more likely it's an outlier)\n",
    "y_scores = -model.score_samples(X_test)  # Negative because lower scores mean more anomalous\n",
    "\n",
    "# Get binary predictions for metrics\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.where(y_pred == -1, 1, 0)  # Convert to 0/1 format\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "\n",
    "# Plot both curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.xlabel('Recall (True Positive Rate)')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall vs ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef, roc_curve, precision_recall_curve\n",
    "\n",
    "def calculate_recall_at_fpr(y_true, y_scores, k=0.005):\n",
    "    \"\"\"Calculate recall at a specific false positive rate.\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    idx = np.argmin(np.abs(fpr - k))\n",
    "    return tpr[idx]\n",
    "\n",
    "def calculate_precision_at_recall(y_true, y_scores, k=0.005):\n",
    "    \"\"\"Calculate precision at a specific recall rate.\"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    idx = np.argmin(np.abs(recall - k))\n",
    "    return precision[idx]\n",
    "\n",
    "def calculate_ks_statistic(y_true, y_scores):\n",
    "    \"\"\"Calculate Kolmogorov-Smirnov statistic.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    return np.max(np.abs(fpr - tpr))\n",
    "\n",
    "# Now calculate the metrics\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "recall_at_k = calculate_recall_at_fpr(y_test, y_scores, k=0.005)\n",
    "precision_at_k = calculate_precision_at_recall(y_test, y_scores, k=0.005)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "ks = calculate_ks_statistic(y_test, y_scores)\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{cm}\")\n",
    "print(f\"Recall at 0.5% FPR: {recall_at_k:.4f}\")\n",
    "print(f\"Precision at 0.5% Recall: {precision_at_k:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "print(f\"Kolmogorov-Smirnov Statistic: {ks:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = model.score_samples(X_test)\n",
    "\n",
    "# Calculate precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "\n",
    "# Plot both curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "plt.plot(fpr, tpr, label='ROC Curve')\n",
    "plt.xlabel('Recall (True Positive Rate)')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall vs ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Isolation Forest model\n",
    "print(\"Training Isolation Forest model...\")\n",
    "if_model = ccf.train_isolation_forest(X_train, config)\n",
    "if_preds, if_scores = ccf.get_model_predictions(if_model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Local Outlier Factor model\n",
    "print(\"Training Local Outlier Factor model...\")\n",
    "lof_model = ccf.train_lof(X_train, config)\n",
    "lof_preds, lof_scores = ccf.get_model_predictions(lof_model, X_test, is_isolation_forest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Isolation Forest\n",
    "if_metrics = ccf.calculate_metrics(y_test, if_preds, if_scores)\n",
    "print(\"Isolation Forest Results:\")\n",
    "print(f\"Accuracy: {if_metrics['accuracy']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(if_metrics['confusion_matrix'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(if_metrics['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LOF\n",
    "lof_metrics = ccf.calculate_metrics(y_test, lof_preds, lof_scores)\n",
    "print(\"Local Outlier Factor Results:\")\n",
    "print(f\"Accuracy: {lof_metrics['accuracy']:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(lof_metrics['confusion_matrix'])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(lof_metrics['classification_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt_roc, auc_if, auc_lof = ccf.plot_roc_curves(y_test, if_scores, lof_scores)\n",
    "plt_roc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "plt_pr = ccf.plot_pr_curves(y_test, if_scores, lof_scores)\n",
    "plt_pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomaly score distributions\n",
    "plt_scores, eval_df = ccf.plot_anomaly_scores(if_scores, lof_scores, y_test)\n",
    "plt_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomaly score comparison\n",
    "plt_comparison = ccf.plot_score_comparison(eval_df)\n",
    "plt_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparisons = ccf.compare_models(y_test, if_preds, lof_preds, if_scores, lof_scores, auc_if, auc_lof)\n",
    "print(\"Model Comparison:\")\n",
    "comparisons.style.highlight_max(axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
